{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Автоэнкодер\n",
    "* Cross-domain\n",
    "* Beam search with lp and cp\n",
    "* SRU\n",
    "* Визуализация Attn\n",
    "* replace_unk по attention'у http://opennmt.net/OpenNMT/translation/unknowns/\n",
    "\n",
    "## Tutorials\n",
    "* http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "* https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "## Articles\n",
    "* Teaching neural networks to point to improve language modeling and translation: https://einstein.ai/research/teaching-neural-networks-to-point-to-improve-language-modeling-and-translation\n",
    "* Training RNNs as Fast as CNNs : https://arxiv.org/abs/1709.02755\n",
    "* Beam Search Strategies for Neural Machine Translation: https://arxiv.org/abs/1702.01806\n",
    "* Unsupervised Machine Translation Using Monolingual Corpora Only: https://arxiv.org/abs/1711.00043\n",
    "* Unsupervised Neural Machine Translation: https://arxiv.org/abs/1710.11041\n",
    "* NIPS 2016 Tutorial: Generative Adversarial Networks: https://arxiv.org/pdf/1701.00160.pdf\n",
    "\n",
    "## Repos\n",
    "* https://github.com/facebookresearch/MUSE\n",
    "* https://github.com/OpenNMT/OpenNMT-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CUDA:  True\n",
      "Collecting vocabularies...\n",
      "Building model...\n",
      "Loading embeddings...\n",
      "Enable training:  False\n",
      "Initializing optimizers...\n",
      "UNMT(\n",
      "  (encoder): EncoderRNN(\n",
      "    (embedding): Embedding(80007, 300)\n",
      "    (rnn): LSTM(300, 200, num_layers=3, dropout=0.1, bidirectional=True)\n",
      "  )\n",
      "  (decoder): AttnDecoderRNN(\n",
      "    (embedding): Embedding(80007, 300)\n",
      "    (attn): Attn(\n",
      "      (attn): Linear(in_features=400, out_features=400)\n",
      "      (sm): Softmax()\n",
      "      (out): Linear(in_features=800, out_features=400)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (rnn): LSTM(700, 400, num_layers=3, dropout=0.1)\n",
      "  )\n",
      "  (generator): Generator(\n",
      "    (out): Linear(in_features=400, out_features=80007)\n",
      "    (sm): LogSoftmax()\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=20000, out_features=1024)\n",
      "      (1): Linear(in_features=1024, out_features=1024)\n",
      "      (2): Linear(in_features=1024, out_features=1024)\n",
      "    )\n",
      "    (out): Linear(in_features=1024, out_features=1)\n",
      "  )\n",
      ")\n",
      "Params:  62203256\n",
      "train.mono.tok.clean.tc.en:   0%|                    | 0.00/470M [00:00<?, ?B/s]Src batch: OneLangBatch: Variable containing:\n",
      "     6   2342     20  ...     153     18    471\n",
      " 30174     19      8  ...       5      3     11\n",
      "   143     31      9  ...      11    132    272\n",
      "        ...            ⋱           ...         \n",
      "     7      4      0  ...       0      0      0\n",
      "     4      2      0  ...       0      0      0\n",
      "     2      0      0  ...       0      0      0\n",
      "[torch.LongTensor of size 33x32]\n",
      ", [33, 32, 29, 28, 27, 22, 21, 21, 20, 16, 15, 14, 13, 13, 13, 11, 10, 10, 10, 9, 9, 9, 8, 8, 7, 7, 7, 6, 6, 6, 6, 5]\n",
      "\n",
      "train.mono.tok.clean.tc.de:   0%|                    | 0.00/485M [00:00<?, ?B/s]\u001b[ATgt batch: OneLangBatch: Variable containing:\n",
      " 40080  40028  40055  ...   69120  40512  40335\n",
      " 44088  40086  40015  ...   40025  40007  40007\n",
      " 40212  40015  46359  ...   40005  40005  40005\n",
      "        ...            ⋱           ...         \n",
      " 40188      0      0  ...       0      0      0\n",
      " 40007      0      0  ...       0      0      0\n",
      " 40005      0      0  ...       0      0      0\n",
      "[torch.LongTensor of size 39x32]\n",
      ", [39, 32, 25, 19, 17, 15, 14, 14, 14, 12, 11, 11, 10, 10, 9, 9, 9, 9, 8, 8, 7, 7, 7, 6, 6, 6, 5, 5, 4, 3, 3, 3]\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> .\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> .\n",
      "0 big epoch, 100 epoch, 135.58942699432373 sec, 12058.2498 main loss, 1.6065 discriminator loss, current losses: [0.6811720728874207, 0.7532814741134644, 0.6562750935554504, 0.7130860090255737, 2481.971923828125, 2124.28759765625, 2481.821044921875, 2116.330322265625]\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py \\\n",
    "    -src_lang en \\\n",
    "    -tgt_lang de \\\n",
    "    -train_src_mono ./data/en-ko/train.mono.tok.clean.tc.en \\\n",
    "    -train_tgt_mono ./data/en-ko/train.mono.tok.clean.tc.de \\\n",
    "    -src_embeddings ./data/en-de/wiki.multi.en.vec \\\n",
    "    -tgt_embeddings ./data/en-de/wiki.multi.de.vec \\\n",
    "    -src_vocabulary ./data/en-de/src.pickle \\\n",
    "    -tgt_vocabulary ./data/en-de/tgt.pickle \\\n",
    "    -all_vocabulary ./data/en-de/all.pickle \\\n",
    "    -src_to_tgt_dict ./data/en-de/en-de.txt \\\n",
    "    -tgt_to_src_dict ./data/en-de/de-en.txt \\\n",
    "    -layers 3 \\\n",
    "    -rnn_size 400 \\\n",
    "    -src_vocab_size 40000 \\\n",
    "    -tgt_vocab_size 40000 \\\n",
    "    -print_every 100 \\\n",
    "    -save_every 100 \\\n",
    "    -batch_size 32 \\\n",
    "    -discriminator_hidden_size 1024 \\\n",
    "    -unsupervised_epochs 5 \\\n",
    "    -supervised_epochs 0 \\\n",
    "    -save_model model \\\n",
    "    -reset_vocabularies 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CUDA:  True\n",
      "Collecting vocabularies...\n",
      "Building model...\n",
      "Initializing optimizers...\n",
      "Writing output...\n"
     ]
    }
   ],
   "source": [
    "!python3 translate.py \\\n",
    "    -src_lang en \\\n",
    "    -tgt_lang de \\\n",
    "    -lang src \\\n",
    "    -model model_new.pt \\\n",
    "    -input ./data/en-de/val.short.en \\\n",
    "    -output ./data/en-de/pred.txt \\\n",
    "    -src_vocabulary ./data/en-de/src.pickle \\\n",
    "    -tgt_vocabulary ./data/en-de/tgt.pickle \\\n",
    "    -all_vocabulary ./data/en-de/all.pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 1.53, 12.9/1.7/0.7/0.3 (BP=1.000, ratio=1.130, hyp_len=251469, ref_len=222496)\r\n",
      "It is in-advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\r\n"
     ]
    }
   ],
   "source": [
    "!perl ./multi-bleu.perl ./data/en-de/val.short.de < ./data/en-de/pred.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
